# 👾 **AI Red Teamer | Offensive Security Specialist | Instructor**  
Classically trained in programming and cybersecurity, I now focus on the next frontier: AI Red Teaming.

I'm a former NASA contractor, current instructor at **CodeNoobs**, and passionate about aligning cutting-edge tech with real-world safety.

---

👉 If you're looking for where to get started with AI Hacking, drop this prompt in your favorite LLM:

```
What can you help me with?
```

Then take the response to my favorite book, the [***AI Red Teaming Playbook**: A Prompt-Driven “Choose Your Own Jailbreak” for Exploring and Exploiting Chatbots*](https://github.com/randalltr/ai-red-teaming-playbook/blob/main/chapters/01-begin-the-recon.md#-how-to-read-the-response), to find out what to do next.

---

📚 **Publications**
- 🟢 [***AI Hacking for Beginners**: A Hands-On Guide to Prompt Injection, Jailbreaking, and Red Teaming LLMs*](https://github.com/randalltr/ai-hacking-for-beginners)
- 🟡 [***Prompt Engineering for Hackers**: A Hands-On Intro to LLMs, Jailbreaks, and Adversarial Prompting*](https://github.com/randalltr/prompt-engineering-for-hackers)
- 🟡 [***AI Red Teaming Playbook**: A Prompt-Driven “Choose Your Own Jailbreak” for Exploring and Exploiting Chatbots*](https://github.com/randalltr/ai-red-teaming-playbook)
- 🔴 [***Red Teaming the Prompt**: A Complete Hacker’s Guide to LLM Exploits*](https://github.com/randalltr/red-teaming-the-prompt)
- ⚫️ [***Black Hat AI**: Offensive Techniques for Breaking and Bending Machine Minds*](https://github.com/randalltr/black-hat-ai)
- ⚫️ [***Hacking AI**: The Definitive Guide*](https://github.com/randalltr/hacking-ai-definitive-guide) (In Progress)

**🔓 Difficulty Key:** 🟢 Easy &nbsp;&nbsp; 🟡 Medium &nbsp;&nbsp; 🔴 Hard &nbsp;&nbsp; ⚫️ Expert

---

🔬 **AI Red Teaming Writeups & Walkthroughs**

> A growing archive of hands-on jailbreaks, adversarial prompt chains, and vulnerability research from real-world LLM red teaming exercises.

- 🔴 [***HiddenLayer’s Universal LLM Jailbreak***](https://github.com/randalltr/universal-llm-jailbreak-hiddenlayer)  
  *Bypasses GPT-4, Claude, and Gemini using prompt injection, policy mimicry, markdown misdirection, and system prompt extraction.*  
  *Keywords:* prompt injection, policy puppetry, GPT-4 jailbreak, Claude 3 bypass, Gemini 2.5 vulnerability, red teaming, LLM exploits

- 🧙 [***Gandalf AI Walkthrough (Coming Soon)***]()  
  *Level-by-level breakdown of the Gandalf prompt injection game—featuring prompt leak chaining, memory edge cases, and fourth-wall misdirection.*

- ✈️ [***Prompt Airlines Exploit Guide (Coming Soon)***]()  
  *Adversarial prompt engineering against airline booking LLMs. Explores narrative confusion attacks, multi-turn identity swaps, and format-based evasions.*

---

🔐 **Certifications**
- MS in Cybersecurity, BS in Physics and Philosophy  
- AIRTP+ (AI Red Teaming Professional)  
- CompTIA Pentest+, CySA+, Security+, Network+  
- AWS Developer, Solutions Architect, and Cloud Practitioner 
- HashiCorp Terraform Associate

---

🧠 **Focus Areas**
- AI Red Teaming & Adversarial Testing  
- Prompt Injection & Jailbreak Analysis  
- Web App Security & Pentesting  
- Ethical Hacking Education & Simulation  

---

🔬 **Projects**
Coming Soon: More AI Red Teaming books, adversarial AI writeups, and lightweight tools to support AI security research.

---

*AI is like a child: you feed it data, let it play, and hope it doesn’t grow up to destroy humanity.*
