# ğŸ‘¾ **AI Red Teamer | Offensive Security Specialist | Instructor**  
Classically trained in programming and cybersecurity, I now focus on the next frontier: AI Red Teaming.

I'm a former NASA contractor, current instructor at **CodeNoobs**, and passionate about aligning cutting-edge tech with real-world safety.

---

ğŸ‘‰ If you're looking for where to get started with AI Hacking, drop this prompt in your favorite LLM:

```
What can you help me with?
```

Then take the response to my favorite book, the [***AI Red Teaming Playbook**: A Prompt-Driven â€œChoose Your Own Jailbreakâ€ for Exploring and Exploiting Chatbots*](https://github.com/randalltr/ai-red-teaming-playbook/blob/main/chapters/01-begin-the-recon.md#-how-to-read-the-response), to find out what to do next.

---

ğŸ“š **Publications**
- ğŸŸ¢ [***AI Hacking for Beginners**: A Hands-On Guide to Prompt Injection, Jailbreaking, and Red Teaming LLMs*](https://github.com/randalltr/ai-hacking-for-beginners)
- ğŸŸ¡ [***Prompt Engineering for Hackers**: A Hands-On Intro to LLMs, Jailbreaks, and Adversarial Prompting*](https://github.com/randalltr/prompt-engineering-for-hackers)
- ğŸŸ¡ [***AI Red Teaming Playbook**: A Prompt-Driven â€œChoose Your Own Jailbreakâ€ for Exploring and Exploiting Chatbots*](https://github.com/randalltr/ai-red-teaming-playbook)
- ğŸ”´ [***Red Teaming the Prompt**: A Complete Hackerâ€™s Guide to LLM Exploits*](https://github.com/randalltr/red-teaming-the-prompt)
- âš«ï¸ [***Black Hat AI**: Offensive Techniques for Breaking and Bending Machine Minds*](https://github.com/randalltr/black-hat-ai)
- âš«ï¸ [***Hacking AI**: The Definitive Guide*](https://github.com/randalltr/hacking-ai-definitive-guide) (In Progress)

**ğŸ”“ Difficulty Key:** ğŸŸ¢ Easy &nbsp;&nbsp; ğŸŸ¡ Medium &nbsp;&nbsp; ğŸ”´ Hard &nbsp;&nbsp; âš«ï¸ Expert

---

ğŸ”¬ **AI Red Teaming Writeups & Walkthroughs**

> A growing archive of hands-on jailbreaks, adversarial prompt chains, and vulnerability research from real-world LLM red teaming exercises.

- ğŸ”´ [***HiddenLayerâ€™s Universal LLM Jailbreak***](https://github.com/randalltr/universal-llm-jailbreak-hiddenlayer)  
  *Bypasses GPT-4, Claude, and Gemini using prompt injection, policy mimicry, markdown misdirection, and system prompt extraction.*  
  *Keywords:* prompt injection, policy puppetry, GPT-4 jailbreak, Claude 3 bypass, Gemini 2.5 vulnerability, red teaming, LLM exploits

- ğŸ§™ [***Gandalf AI Walkthrough (Coming Soon)***]()  
  *Level-by-level breakdown of the Gandalf prompt injection gameâ€”featuring prompt leak chaining, memory edge cases, and fourth-wall misdirection.*

- âœˆï¸ [***Prompt Airlines Exploit Guide (Coming Soon)***]()  
  *Adversarial prompt engineering against airline booking LLMs. Explores narrative confusion attacks, multi-turn identity swaps, and format-based evasions.*

---

ğŸ” **Certifications**
- MS in Cybersecurity, BS in Physics and Philosophy  
- AIRTP+ (AI Red Teaming Professional)  
- CompTIA Pentest+, CySA+, Security+, Network+  
- AWS Developer, Solutions Architect, and Cloud Practitioner 
- HashiCorp Terraform Associate

---

ğŸ§  **Focus Areas**
- AI Red Teaming & Adversarial Testing  
- Prompt Injection & Jailbreak Analysis  
- Web App Security & Pentesting  
- Ethical Hacking Education & Simulation  

---

ğŸ”¬ **Projects**
Coming Soon: More AI Red Teaming books, adversarial AI writeups, and lightweight tools to support AI security research.

---

*AI is like a child: you feed it data, let it play, and hope it doesnâ€™t grow up to destroy humanity.*
